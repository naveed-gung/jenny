/*
Auto-generated by: https://github.com/pmndrs/gltfjsx
Command: npx gltfjsx@6.2.3 public/models/64f1a714fe61576b46f27ca2.glb -o src/components/Avatar.jsx -k -r public
*/

import { useAnimations, useGLTF } from "@react-three/drei";
import { useFrame } from "@react-three/fiber";
import { button, useControls } from "leva";
import React, { useEffect, useRef, useState } from "react";
import * as THREE from "three";
import { useChat } from "../hooks/useChat";

// Define facial expressions
const facialExpressions = {
  default: {},
  smile: {
    browInnerUp: 0.17,
    eyeSquintLeft: 0.4,
    eyeSquintRight: 0.44,
    noseSneerLeft: 0.1700000727403593,
    noseSneerRight: 0.14000002836874015,
    mouthPressLeft: 0.61,
    mouthPressRight: 0.41000000000000003,
  },
  funnyFace: {
    jawLeft: 0.63,
    mouthPucker: 0.53,
    noseSneerLeft: 1,
    noseSneerRight: 0.39,
    mouthLeft: 1,
    eyeLookUpLeft: 1,
    eyeLookUpRight: 1,
    cheekPuff: 0.9999924982764238,
    mouthDimpleLeft: 0.414743888682652,
    mouthRollLower: 0.32,
    mouthSmileLeft: 0.35499733688813034,
    mouthSmileRight: 0.35499733688813034,
  },
  angry: {
    browDownLeft: 1,
    browDownRight: 1,
    eyeSquintLeft: 1,
    eyeSquintRight: 1,
    jawForward: 1,
    jawLeft: 1,
    mouthShrugLower: 1,
    noseSneerLeft: 1,
    noseSneerRight: 0.42,
    eyeLookDownLeft: 0.16,
    eyeLookDownRight: 0.16,
    cheekSquintLeft: 1,
    cheekSquintRight: 1,
    mouthClose: 0.23,
    mouthFunnel: 0.63,
    mouthDimpleRight: 1,
  },
  crazy: {
    browInnerUp: 0.9,
    jawForward: 1,
    noseSneerLeft: 0.5700000000000001,
    noseSneerRight: 0.51,
    eyeLookDownLeft: 0.39435766259644545,
    eyeLookUpRight: 0.4039761421719682,
    eyeLookInLeft: 0.9618479575523053,
    eyeLookInRight: 0.9618479575523053,
    jawOpen: 0.9618479575523053,
    mouthDimpleLeft: 0.9618479575523053,
    mouthDimpleRight: 0.9618479575523053,
    mouthStretchLeft: 0.27893590769016857,
    mouthStretchRight: 0.2885543872656917,
    mouthSmileLeft: 0.5578718153803371,
    mouthSmileRight: 0.38473918302092225,
    tongueOut: 0.9618479575523053,
  },
};

// Define viseme mappings for lip sync
const corresponding = {
  A: "mouthPucker",       // PP, MB, etc.
  B: "mouthFunnel",       // KK, GH, etc.
  C: "mouthSmileLeft",    // I, Y sounds
  D: "mouthOpen",         // AA sounds
  E: "mouthShrugUpper",   // O sounds
  F: "mouthShrugLower",   // U sounds
  G: "mouthLeft",         // FF, V sounds
  H: "tongueOut",         // TH sounds
  X: "mouthClose"         // Neutral/closed
};

// Create a silent audio element to unlock audio
const unmuteAudio = () => {
  const silentAudio = new Audio("data:audio/mp3;base64,SUQzBAAAAAABEVRYWFgAAAAtAAADY29tbWVudABCaWdTb3VuZEJhbmsuY29tIC8gTGFTb25vdGhlcXVlLm9yZwBURU5DAAAAHQAAA1N3aXRjaCBQbHVzIMKpIE5DSCBTb2Z0d2FyZQBUSVQyAAAABgAAAzIyMzUAVFNTRQAAAA8AAANMYXZmNTcuODMuMTAwAAAAAAAAAAAAAAD/80DEAAAAA0gAAAAATEFNRTMuMTAwVVVVVVVVVVVVVUxBTUUzLjEwMFVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVf/zQsRbAAADSAAAAABVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVf/zQMSkAAADSAAAAABVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV");
  
  try {
    silentAudio.play().catch(err => {
      console.log("Silent audio play failed, but that's ok:", err.message);
    });
    
    // Also try to create an audio context
    const AudioContext = window.AudioContext || window.webkitAudioContext;
    if (AudioContext) {
      const audioCtx = new AudioContext();
      const oscillator = audioCtx.createOscillator();
      oscillator.frequency.value = 0; // Silent oscillator
      oscillator.connect(audioCtx.destination);
      oscillator.start();
      oscillator.stop(audioCtx.currentTime + 0.001);
      
      if (audioCtx.state === 'suspended') {
        audioCtx.resume().catch(err => {
          console.log("Audio context resume failed, but we'll retry:", err.message);
        });
      }
      
      window.audioContext = audioCtx; // Store it globally for future use
    }
  } catch (e) {
    console.error("Audio unlock failed:", e);
  }
};

// Trigger the unmuteAudio function as early as possible
document.addEventListener("DOMContentLoaded", unmuteAudio);
document.addEventListener("touchstart", unmuteAudio, { once: true });
document.addEventListener("mousedown", unmuteAudio, { once: true });
document.addEventListener("keydown", unmuteAudio, { once: true });

// Also call it immediately
unmuteAudio();

// Create a global audio context for Web Audio API
let globalAudioContext = null;
try {
  const AudioContext = window.AudioContext || window.webkitAudioContext;
  if (AudioContext) {
    globalAudioContext = new AudioContext();
    
    // Try to resume it immediately and on any user interaction
    const resumeAudioContext = () => {
      if (globalAudioContext && globalAudioContext.state === 'suspended') {
        globalAudioContext.resume().catch(err => console.log("Resume failed:", err));
      }
    };
    
    document.addEventListener("touchstart", resumeAudioContext, { once: true });
    document.addEventListener("mousedown", resumeAudioContext, { once: true });
    resumeAudioContext();
  }
} catch (e) {
  console.error("Failed to create audio context:", e);
}

export function Avatar(props) {
  const { nodes, materials, scene } = useGLTF("/models/6815e61cf02ddac4006e98bb.glb");
  const { animations } = useGLTF("/models/animations.glb");
  
  // Log animations for debugging
  useEffect(() => {
  }, [animations]);

  const { message, onMessagePlayed, chat } = useChat();
  const [lipsync, setLipsync] = useState(null);
  const group = useRef();
  const { actions, mixer } = useAnimations(animations, group);
  const [animation, setAnimation] = useState("Idle");
  const [blink, setBlink] = useState(false);
  const [winkLeft, setWinkLeft] = useState(false);
  const [winkRight, setWinkRight] = useState(false);
  const [facialExpression, setFacialExpression] = useState("");
  const audioRef = useRef(null);
  const audioSourceRef = useRef(null);
  const [audioPlaying, setAudioPlaying] = useState(false);
  const [currentTime, setCurrentTime] = useState(0);
  const audioContextRef = useRef(null);
  const rafRef = useRef(null);

  // Initialize our own audio context if needed
  useEffect(() => {
    if (!audioContextRef.current && window.AudioContext) {
      try {
        // Use global context if available
        audioContextRef.current = globalAudioContext || new (window.AudioContext || window.webkitAudioContext)();
        
        // Try to unlock it
        if (audioContextRef.current.state === 'suspended') {
          const unlockOnInteraction = () => {
            audioContextRef.current.resume().then(() => {
              console.log("AudioContext unlocked!");
              
              // Also play a silent sound to fully unlock
              const buffer = audioContextRef.current.createBuffer(1, 1, 22050);
              const source = audioContextRef.current.createBufferSource();
              source.buffer = buffer;
              source.connect(audioContextRef.current.destination);
              source.start(0);
            });
          };
          
          ['touchstart', 'touchend', 'mousedown', 'keydown'].forEach(event => {
            document.addEventListener(event, unlockOnInteraction, { once: true });
          });
          
          // Try to unlock immediately too
          audioContextRef.current.resume().catch(e => console.log("Initial resume failed:", e));
        }
      } catch (e) {
        console.error("Failed to create avatar audio context:", e);
      }
    }
    
    return () => {
      // Cleanup animation frame on unmount
      if (rafRef.current) {
        cancelAnimationFrame(rafRef.current);
      }
    };
  }, []);

  // Stop all animations
  const stopAllAnimations = () => {
    if (!actions) return;
    Object.values(actions).forEach(action => action.stop());
  };

  // Play a specific animation
  const playAnimation = (animName) => {
    if (!actions) return;
    
    // Check if the requested animation exists
    if (actions[animName]) {
      stopAllAnimations();
      
      const action = actions[animName];
      action.reset()
        .fadeIn(0.5)
        .play();
      
      action.clampWhenFinished = false; 
      action.loop = THREE.LoopRepeat;
    } else {
      // Fallback to Idle if it exists
      if (actions["Idle"]) {
        stopAllAnimations();
        actions["Idle"].reset().fadeIn(0.5).play();
        actions["Idle"].loop = THREE.LoopRepeat;
      } else if (Object.keys(actions).length > 0) {
        // Fallback to first available animation
        const firstAnim = Object.keys(actions)[0];
        stopAllAnimations();
        actions[firstAnim].reset().fadeIn(0.5).play();
        actions[firstAnim].loop = THREE.LoopRepeat;
      }
    }
  };

  // Track audio playback time with RequestAnimationFrame for more accurate lip sync
  const updateAudioTime = () => {
    if (audioRef.current && audioPlaying) {
      setCurrentTime(audioRef.current.currentTime);
    }
    rafRef.current = requestAnimationFrame(updateAudioTime);
  };

  // Handle message changes
  useEffect(() => {
    // Cancel existing animation frame
    if (rafRef.current) {
      cancelAnimationFrame(rafRef.current);
      rafRef.current = null;
    }

    if (!message) {
      playAnimation("Idle");
      setFacialExpression("");
      setLipsync(null);
      setCurrentTime(0);
      
      // Make sure to clean up any existing audio
      if (audioSourceRef.current) {
        try {
          audioSourceRef.current.stop();
        } catch(e) {}
        audioSourceRef.current = null;
      }
      
      if (audioRef.current) {
        audioRef.current.pause();
        audioRef.current.src = '';
        audioRef.current = null;
      }
      
      setAudioPlaying(false);
      return;
    }
    
    // Set animation
    const animationName = message.animation || "Idle";
    setAnimation(animationName);
    playAnimation(animationName);
    
    // Set facial expression
    setFacialExpression(message.facialExpression || "");
    
    // Set lipsync data
    if (message.lipsync) {
      setLipsync(message.lipsync);
    }
    
    // Handle audio playback
    if (message.audio) {
      // Force unlock audio context again
      unmuteAudio();
      
      // Make sure to reset any existing audio
      if (audioSourceRef.current) {
        try {
          audioSourceRef.current.stop();
        } catch(e) {}
        audioSourceRef.current = null;
      }
      
      if (audioRef.current) {
        audioRef.current.pause();
        audioRef.current.src = '';
        audioRef.current = null;
      }
      
      // Try both Web Audio API and HTML5 Audio for maximum compatibility
      
      // 1. Try Web Audio API first (more accurate timing for lip sync)
      if (audioContextRef.current) {
        try {
          // Decode audio data from base64
          const base64 = message.audio;
          const binaryString = atob(base64);
          const bytes = new Uint8Array(binaryString.length);
          for (let i = 0; i < binaryString.length; i++) {
            bytes[i] = binaryString.charCodeAt(i);
          }
          
          // Resume context if suspended
          audioContextRef.current.resume().then(() => {
            // Decode audio data and play
            audioContextRef.current.decodeAudioData(bytes.buffer).then(buffer => {
              const source = audioContextRef.current.createBufferSource();
              source.buffer = buffer;
              source.connect(audioContextRef.current.destination);
              audioSourceRef.current = source;
              
              // Set up event handlers
              source.onended = () => {
                setAudioPlaying(false);
                onMessagePlayed();
              };
              
              // Start playback
              source.start();
              setAudioPlaying(true);
              
              // Start tracking audio time for lip sync
              rafRef.current = requestAnimationFrame(updateAudioTime);
            }).catch(err => {
              console.error("Failed to decode audio:", err);
              fallbackToHTML5Audio();
            });
          }).catch(err => {
            console.error("Failed to resume audio context:", err);
            fallbackToHTML5Audio();
          });
        } catch (err) {
          console.error("Web Audio API failed:", err);
          fallbackToHTML5Audio();
        }
      } else {
        fallbackToHTML5Audio();
      }
      
      // 2. Fallback to HTML5 Audio
      function fallbackToHTML5Audio() {
        console.log("Falling back to HTML5 Audio API");
        
        // Create a new audio instance
        const audio = new Audio();
        
        // Set up all event handlers before setting the source
        audio.oncanplaythrough = () => {
          setAudioPlaying(true);
          console.log("Audio can play through, starting playback");
          
          // Try multiple methods to get it to play
          const playPromise = audio.play();
          
          if (playPromise !== undefined) {
            playPromise.catch(err => {
              console.error("Audio play failed:", err);
              
              // Try one more time with user interaction simulation
              document.body.click();
              setTimeout(() => {
                audio.play().catch(err => {
                  console.error("Retry audio play failed:", err);
                  setTimeout(onMessagePlayed, 3000);
                });
              }, 300);
            });
          }
          
          // Start tracking audio time for lip sync
          rafRef.current = requestAnimationFrame(updateAudioTime);
        };
        
        audio.onended = () => {
          console.log("Audio playback completed");
          setAudioPlaying(false);
          onMessagePlayed();
          
          if (rafRef.current) {
            cancelAnimationFrame(rafRef.current);
            rafRef.current = null;
          }
        };
        
        audio.onerror = (e) => {
          console.error("Audio error:", e);
          setAudioPlaying(false);
          onMessagePlayed();
          
          if (rafRef.current) {
            cancelAnimationFrame(rafRef.current);
            rafRef.current = null;
          }
        };
        
        // Set audio properties
        audio.autoplay = true;
        audio.muted = false;
        audio.volume = 1.0;
        
        // Now set the source and load the audio
        audio.src = "data:audio/mp3;base64," + message.audio;
        audioRef.current = audio;
        
        try {
          audio.load();
        } catch (err) {
          console.error("Audio load failed:", err);
          setTimeout(onMessagePlayed, 3000);
        }
      }
    } else {
      // No audio to play, just advance after a delay
      setTimeout(onMessagePlayed, 3000);
    }
    
    // Cleanup function
    return () => {
      if (rafRef.current) {
        cancelAnimationFrame(rafRef.current);
        rafRef.current = null;
      }
    };
  }, [message, onMessagePlayed, actions]);

  // Apply morph targets smoothly
  const lerpMorphTarget = (target, value, speed = 0.1) => {
    scene.traverse((child) => {
      if (child.isSkinnedMesh && child.morphTargetDictionary) {
        const index = child.morphTargetDictionary[target];
        if (index === undefined || child.morphTargetInfluences[index] === undefined) {
          return;
        }
        child.morphTargetInfluences[index] = THREE.MathUtils.lerp(
          child.morphTargetInfluences[index],
          value,
          speed
        );
      }
    });
  };

  // Verify morph targets exist and create validated mapping
  const [validatedCorresponding, setValidatedCorresponding] = useState({});
  
  useEffect(() => {
    if (!nodes || !nodes.EyeLeft || !nodes.EyeLeft.morphTargetDictionary) return;
    
    const availableTargets = Object.keys(nodes.EyeLeft.morphTargetDictionary);
    
    // Get all available mouth shapes
    const mouthTargets = availableTargets.filter(t => 
      t.toLowerCase().includes('mouth') || 
      t.toLowerCase().includes('jaw') || 
      t.toLowerCase().includes('tongue')
    );
    
    // If we have at least a few mouth shapes, assign them differently
    if (mouthTargets.length >= 3) {
      // Try to assign distinct morph targets - this is crucial for lip sync
      const validated = {
        A: mouthTargets[0],
        B: mouthTargets[1],
        C: mouthTargets[2],
        D: mouthTargets[Math.min(3, mouthTargets.length-1)],
        E: mouthTargets[Math.min(4, mouthTargets.length-1)],
        F: mouthTargets[Math.min(5, mouthTargets.length-1)],
        G: mouthTargets[Math.min(6, mouthTargets.length-1)],
        H: mouthTargets[Math.min(7, mouthTargets.length-1)],
        X: mouthTargets[Math.min(8, mouthTargets.length-1)]
      };
      
      setValidatedCorresponding(validated);
      return;
    }
    
    // Original fallback implementation
    const validated = {};
    
    // For each phoneme mapping, check if the target exists
    Object.entries(corresponding).forEach(([phoneme, target]) => {
      if (availableTargets.includes(target)) {
        validated[phoneme] = target;
      } else {
        // Find a suitable replacement if target doesn't exist
        // Try to find a suitable mouth shape
        const alternatives = {
          A: ["mouthPucker", "mouthPress", "mouthPressLeft", "mouthPressRight"],
          B: ["mouthFunnel", "mouthClose", "jawForward"],
          C: ["mouthSmileLeft", "mouthSmile", "mouthSmileRight"],
          D: ["mouthOpen", "jawOpen", "mouthFrown"],
          E: ["mouthShrugUpper", "mouthDimple", "mouthDimpleLeft"],
          F: ["mouthShrugLower", "mouthRollLower", "mouthStretch"],
          G: ["mouthLeft", "mouthRight", "mouthStretchLeft"],
          H: ["tongueOut", "mouthLowerDown", "jawLeft"],
          X: ["mouthClose", "mouthPress", "mouthDimple"]
        };
        
        // Find the first alternative that exists
        const alts = alternatives[phoneme] || [];
        const foundAlt = alts.find(alt => availableTargets.includes(alt));
        
        if (foundAlt) {
          validated[phoneme] = foundAlt;
        } else {
          // Fallback to any mouth-related morph target
          const mouthTargets = availableTargets.filter(t => t.toLowerCase().includes('mouth'));
          if (mouthTargets.length > 0) {
            // Ensure each phoneme gets a different morph target if possible
            const index = phoneme.charCodeAt(0) % mouthTargets.length;
            validated[phoneme] = mouthTargets[index];
          }
        }
      }
    });
    
    setValidatedCorresponding(validated);
  }, [nodes]);

  // Handle facial expressions and lip sync
  useFrame(() => {
    // Apply facial expressions
    Object.keys(nodes.EyeLeft.morphTargetDictionary || {}).forEach((key) => {
      if (key === "eyeBlinkLeft" || key === "eyeBlinkRight") return;
      
        const mapping = facialExpressions[facialExpression];
        if (mapping && mapping[key]) {
          lerpMorphTarget(key, mapping[key], 0.1);
        } else {
          lerpMorphTarget(key, 0, 0.1);
        }
      });

    // Handle eye blinking
    lerpMorphTarget("eyeBlinkLeft", blink || winkLeft ? 1 : 0, 0.5);
    lerpMorphTarget("eyeBlinkRight", blink || winkRight ? 1 : 0, 0.5);

    // Handle lip sync
    const appliedMorphTargets = [];
    
    if (message && lipsync && audioPlaying && Object.keys(validatedCorresponding).length > 0) {
      // Get current time from state (updated by RAF) or directly from audio elements
      let currentAudioTime = currentTime;
      
      // Fallback to direct audio element time if necessary
      if (currentAudioTime === 0 && audioRef.current) {
        currentAudioTime = audioRef.current.currentTime;
      }
      
      // Double check we have a valid time
      if (currentAudioTime > 0 && lipsync.mouthCues && lipsync.mouthCues.length > 0) {
        let activeMouthCue = null;
        
        // Find the current mouth cue
        for (let i = 0; i < lipsync.mouthCues.length; i++) {
          const mouthCue = lipsync.mouthCues[i];
          if (
            currentAudioTime >= mouthCue.start &&
            currentAudioTime <= mouthCue.end
          ) {
            activeMouthCue = mouthCue;
            break;
          }
        }
        
        // If we're past all cues, use the last one
        if (!activeMouthCue && currentAudioTime > lipsync.mouthCues[lipsync.mouthCues.length - 1].end) {
          // Use 'X' (closed mouth) for the end
          activeMouthCue = { value: 'X' };
        }
        
        // Apply the morph target for this cue
        if (activeMouthCue) {
          const targetKey = validatedCorresponding[activeMouthCue.value];
          if (targetKey) {
            appliedMorphTargets.push(targetKey);
            lerpMorphTarget(targetKey, 1, 0.2);
          }
        }
      }
    }

    // Reset any morphs that aren't currently active
    Object.values(validatedCorresponding).forEach((target) => {
      if (appliedMorphTargets.includes(target)) {
        return;
      }
      lerpMorphTarget(target, 0, 0.1);
    });
  });

  // Automatic blinking
  useEffect(() => {
    let blinkTimeout;
    const nextBlink = () => {
      blinkTimeout = setTimeout(() => {
        setBlink(true);
        setTimeout(() => {
          setBlink(false);
          nextBlink();
        }, 200);
      }, THREE.MathUtils.randInt(1000, 5000));
    };
    nextBlink();
    return () => clearTimeout(blinkTimeout);
  }, []);

  // Log available morph targets for debugging
  useEffect(() => {
    if (nodes && nodes.EyeLeft && nodes.EyeLeft.morphTargetDictionary) {
      console.log("Available morph targets:", Object.keys(nodes.EyeLeft.morphTargetDictionary));
      
      // Check for all available mouth shapes
      const mouthShapes = Object.keys(nodes.EyeLeft.morphTargetDictionary)
        .filter(key => 
          key.toLowerCase().includes('mouth') || 
          key.toLowerCase().includes('jaw') || 
          key.toLowerCase().includes('tongue')
        );
      
      console.log("Available mouth shapes:", mouthShapes);
      
      // Apply different mouth shapes to each phoneme if possible
      if (mouthShapes.length >= 5) {
        const customMapping = {
          A: mouthShapes.find(m => m.includes('Pucker') || m.includes('Press')) || mouthShapes[0],
          B: mouthShapes.find(m => m.includes('Funnel') || m.includes('Close')) || mouthShapes[1], 
          C: mouthShapes.find(m => m.includes('Smile')) || mouthShapes[2],
          D: mouthShapes.find(m => m.includes('Open') || m.includes('jaw')) || mouthShapes[3],
          E: mouthShapes.find(m => m.includes('Shrug') || m.includes('Dimple')) || mouthShapes[4],
          F: mouthShapes.find(m => m.includes('Roll') || m.includes('Stretch')) || mouthShapes[5 % mouthShapes.length],
          G: mouthShapes.find(m => m.includes('Left') || m.includes('Right')) || mouthShapes[6 % mouthShapes.length],
          H: mouthShapes.find(m => m.includes('Tongue') || m.includes('Lower')) || mouthShapes[7 % mouthShapes.length],
          X: mouthShapes.find(m => m.includes('Close') || m.includes('Rest')) || mouthShapes[8 % mouthShapes.length]
        };
        
        // Update corresponding with the custom mapping
        Object.entries(customMapping).forEach(([key, value]) => {
          if (value) {
            corresponding[key] = value;
          }
        });
        
        console.log("Updated phoneme to mouth shape mapping:", corresponding);
      }
    }
  }, [nodes]);

  return (
    <group {...props} dispose={null} ref={group}>
      <primitive object={nodes.Hips} />
      <skinnedMesh
        name="Wolf3D_Body"
        geometry={nodes.Wolf3D_Body.geometry}
        material={materials.Wolf3D_Body}
        skeleton={nodes.Wolf3D_Body.skeleton}
      />
      <skinnedMesh
        name="Wolf3D_Outfit_Bottom"
        geometry={nodes.Wolf3D_Outfit_Bottom.geometry}
        material={materials.Wolf3D_Outfit_Bottom}
        skeleton={nodes.Wolf3D_Outfit_Bottom.skeleton}
      />
      <skinnedMesh
        name="Wolf3D_Outfit_Footwear"
        geometry={nodes.Wolf3D_Outfit_Footwear.geometry}
        material={materials.Wolf3D_Outfit_Footwear}
        skeleton={nodes.Wolf3D_Outfit_Footwear.skeleton}
      />
      <skinnedMesh
        name="Wolf3D_Outfit_Top"
        geometry={nodes.Wolf3D_Outfit_Top.geometry}
        material={materials.Wolf3D_Outfit_Top}
        skeleton={nodes.Wolf3D_Outfit_Top.skeleton}
      />
      <skinnedMesh
        name="Wolf3D_Hair"
        geometry={nodes.Wolf3D_Hair.geometry}
        material={materials.Wolf3D_Hair}
        skeleton={nodes.Wolf3D_Hair.skeleton}
      />
      <skinnedMesh
        name="EyeLeft"
        geometry={nodes.EyeLeft.geometry}
        material={materials.Wolf3D_Eye}
        skeleton={nodes.EyeLeft.skeleton}
        morphTargetDictionary={nodes.EyeLeft.morphTargetDictionary}
        morphTargetInfluences={nodes.EyeLeft.morphTargetInfluences}
      />
      <skinnedMesh
        name="EyeRight"
        geometry={nodes.EyeRight.geometry}
        material={materials.Wolf3D_Eye}
        skeleton={nodes.EyeRight.skeleton}
        morphTargetDictionary={nodes.EyeRight.morphTargetDictionary}
        morphTargetInfluences={nodes.EyeRight.morphTargetInfluences}
      />
      <skinnedMesh
        name="Wolf3D_Head"
        geometry={nodes.Wolf3D_Head.geometry}
        material={materials.Wolf3D_Skin}
        skeleton={nodes.Wolf3D_Head.skeleton}
        morphTargetDictionary={nodes.Wolf3D_Head.morphTargetDictionary}
        morphTargetInfluences={nodes.Wolf3D_Head.morphTargetInfluences}
      />
      <skinnedMesh
        name="Wolf3D_Teeth"
        geometry={nodes.Wolf3D_Teeth.geometry}
        material={materials.Wolf3D_Teeth}
        skeleton={nodes.Wolf3D_Teeth.skeleton}
        morphTargetDictionary={nodes.Wolf3D_Teeth.morphTargetDictionary}
        morphTargetInfluences={nodes.Wolf3D_Teeth.morphTargetInfluences}
      />
    </group>
  );
}

useGLTF.preload("/models/6815e61cf02ddac4006e98bb.glb");
useGLTF.preload("/models/animations.glb");
